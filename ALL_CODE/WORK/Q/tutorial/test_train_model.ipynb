{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db50a5a3-7fc0-47a3-968e-c7762c48bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a01c4e71-ead5-4814-b951-15fe2509f534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/skm/SKM/WORK/ALL_CODE/WORK/Q/tutorial',\n",
       " '/home/skm/.vscode/extensions/ms-toolsai.jupyter-2022.9.1202862440/pythonFiles',\n",
       " '/home/skm/.vscode/extensions/ms-toolsai.jupyter-2022.9.1202862440/pythonFiles/lib/python',\n",
       " '/home/skm/anaconda3/envs/tmpp/lib/python38.zip',\n",
       " '/home/skm/anaconda3/envs/tmpp/lib/python3.8',\n",
       " '/home/skm/anaconda3/envs/tmpp/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/home/skm/anaconda3/envs/tmpp/lib/python3.8/site-packages',\n",
       " '../',\n",
       " '../',\n",
       " '../',\n",
       " '../',\n",
       " '../',\n",
       " '../',\n",
       " '../']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd8c3dbd-8b14-4e86-9283-9741381ff645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "from scripts.train import norm_train\n",
    "from models.metrics import iou, dice_coef\n",
    "from models.callback.save_best import SavebestweightsandEarlyStopping\n",
    "\n",
    "from models.loss import *\n",
    "from models.core import *\n",
    "from models.metrics import *\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall, CategoricalAccuracy\n",
    "\n",
    "# Setup giới hạn vram sử dụng làm hạn chếviệc tràn vram khi trainning\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4800)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "def main(mission, use_model, img_path, shp_path, box_path, old_weights):\n",
    "    \n",
    "    # Đọc dữ liệu prameters từ file json\n",
    "    root_dir = os.path.dirname(sys.path[0]) \n",
    "    config_path = os.path.join(root_dir, 'configs', '%s.json'%(use_model))\n",
    "    dict_params = json.load(open(config_path))\n",
    "\n",
    "    total_stragegy = {}\n",
    "    total_stragegy.update(dict_params['data'])\n",
    "    total_stragegy.update(dict_params['strategy'])\n",
    "    del dict_params['data'], dict_params['strategy'], dict_params['predict']\n",
    "    \n",
    "    use_model = dict_params['name']\n",
    "    init_model = eval(use_model)\n",
    "    model = init_model(**dict_params)\n",
    "    # model.load_weights('/home/quyet/DATA_ML/WorkSpace/segmentation/weights/att_unet/att_unet_forest_monitor_2_512_5class_train.h5')\n",
    "\n",
    "    losses_func = []\n",
    "    for i in total_stragegy['losses']:\n",
    "        losses_func.append(eval(i))\n",
    "\n",
    "    model_metrics = []\n",
    "    for j in total_stragegy['metrics']:\n",
    "        model_metrics.append(eval(j))\n",
    "\n",
    "    optimizer = eval(total_stragegy['optimizer'])\n",
    "\n",
    "    print(\"Init callback function\")\n",
    "    def lr_decay(epoch):\n",
    "        if epoch < 1:\n",
    "            return total_stragegy['init_loss']\n",
    "        else:\n",
    "            return total_stragegy['init_loss'] * 0.98 ** (epoch)\n",
    "\n",
    "    data_path = os.path.join(os.path.join(root_dir, 'data', mission))\n",
    "    checkpoint_filepath = os.path.join(root_dir, 'logs', mission, 'tmp')\n",
    "    log_dir = os.path.join(root_dir, 'logs', mission, 'graph')\n",
    "    weights_path = os.path.join(root_dir, 'weights', '%s'%(use_model), use_model+'_'+mission+'_'+str(total_stragegy['img_size'])+'_'+str(dict_params['n_labels'])+'class.h5')\n",
    "    patience = 10\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only= True, \n",
    "                                                                    monitor='val_loss', mode='min', save_best_only=True)\n",
    "    model_lrscheduler_callback = tf.keras.callbacks.LearningRateScheduler(lr_decay, verbose=1)\n",
    "    model_lrreduce_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience, min_lr=1e-7, verbose=1)\n",
    "    model_earlystopping_callback = SavebestweightsandEarlyStopping(patience=patience, weights_path=weights_path)\n",
    "    model_endtrainnan_callback = tf.keras.callbacks.TerminateOnNaN()\n",
    "    model_tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_images=True)\n",
    "    model_callbacks = [model_checkpoint_callback, model_lrscheduler_callback,\n",
    "                        model_lrreduce_callback, model_earlystopping_callback,\n",
    "                        model_tensorboard_callback,]\n",
    "\n",
    "    history = norm_train(model, optimizer, losses_func, total_stragegy['loss_weights'], model_callbacks , model_metrics, \n",
    "                         data_path, img_path, shp_path, box_path, total_stragegy['img_size'], total_stragegy['num_band'], \n",
    "                         total_stragegy['epochs'], total_stragegy['batch_size'], dict_params['n_labels'], total_stragegy['split_ratios'], \n",
    "                         total_stragegy['use_test'], total_stragegy['use_multi'], old_weights, total_stragegy[\"img_size\"], total_stragegy[\"stride_crop\"])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f604fce-c3c8-4119-b66a-cced39ffc9e4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init callback function\n",
      "/home/skm/SKM16/Data/ZZ/img/T1.tif/home/skm/SKM16/Data/ZZ/img/T2.tif/home/skm/SKM16/Data/ZZ/img/T8.tif/home/skm/SKM16/Data/ZZ/img/S2A_MSIL1C_20210301T031641_N0209_R118_T48NUG_20210301T062115.tif\n",
      "\n",
      "\n",
      "\n",
      "GEOGCS[\"WGS 84\",DATUM[\"World Geodetic System 1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST]]GEOGCS[\"WGS 84\",DATUM[\"World Geodetic System 1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST]]\n",
      "\n",
      "GEOGCS[\"WGS 84\",DATUM[\"World Geodetic System 1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST]]\n",
      "PROJCS[\"WGS 84 / UTM zone 48N\",GEOGCS[\"WGS 84\",DATUM[\"World Geodetic System 1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_identify: /home/skm/anaconda3/envs/tmpp/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation.\n",
      "ERROR 1: PROJ: proj_identify: /home/skm/anaconda3/envs/tmpp/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation.\n",
      "ERROR 1: PROJ: proj_identify: /home/skm/anaconda3/envs/tmpp/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation.\n",
      "ERROR 1: PROJ: proj_identify: /home/skm/anaconda3/envs/tmpp/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation.\n",
      "ERROR 1: PROJ: proj_identify: /home/skm/anaconda3/envs/tmpp/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation.\n",
      "ERROR 1: PROJ: proj_identify: /home/skm/anaconda3/envs/tmpp/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation.\n",
      "ERROR 1: PROJ: proj_identify: /home/skm/anaconda3/envs/tmpp/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation.\n",
      "ERROR 1: PROJ: proj_identify: /home/skm/anaconda3/envs/tmpp/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation.\n",
      "ERROR 1: PROJ: proj_identify: /home/skm/anaconda3/envs/tmpp/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation.\n",
      "ERROR 1: PROJ: proj_identify: /home/skm/anaconda3/envs/tmpp/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation.\n",
      "ERROR 1: PROJ: proj_identify: /home/skm/anaconda3/envs/tmpp/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation.\n",
      "ERROR 1: PROJ: proj_identify: /home/skm/anaconda3/envs/tmpp/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation.\n"
     ]
    },
    {
     "ename": "RasterioIOError",
     "evalue": "Read or write failed. PROJ: proj_identify: /home/skm/anaconda3/envs/tmpp/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"rasterio/_io.pyx\", line 928, in rasterio._io.DatasetReaderBase._read\n  File \"rasterio/_base.pyx\", line 1218, in rasterio._base.DatasetBase.colorinterp.__get__\n  File \"rasterio/_err.pyx\", line 191, in rasterio._err.exc_wrap_int\nrasterio._err.CPLE_AppDefinedError: PROJ: proj_identify: /home/skm/anaconda3/envs/tmpp/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/skm/anaconda3/envs/tmpp/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/skm/anaconda3/envs/tmpp/lib/python3.8/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"/home/skm/SKM/WORK/ALL_CODE/WORK/Q/tutorial/../preprocess/build_mask.py\", line 54, in build_mask\n    img_filter = src.read_masks(1)\n  File \"rasterio/_io.pyx\", line 819, in rasterio._io.DatasetReaderBase.read_masks\n  File \"rasterio/_io.pyx\", line 939, in rasterio._io.DatasetReaderBase._read\nrasterio.errors.RasterioIOError: Read or write failed. PROJ: proj_identify: /home/skm/anaconda3/envs/tmpp/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 29\u001b[0m\n\u001b[1;32m     22\u001b[0m old_weights \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m# img_path = '/home/quyet/DATA_ML/WorkSpace/segmentation/tutorial/LuyenAll/images'\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m# shp_path = '/home/quyet/DATA_ML/WorkSpace/segmentation/tutorial/LuyenAll/lables_water'\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m# box_path = '/home/quyet/DATA_ML/WorkSpace/segmentation/tutorial/LuyenAll/boxs'\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m# old_weights = '/home/quyet/DATA_ML/WorkSpace/segmentation/tutorial/LuyenAll/water_weights.h5'\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m history \u001b[39m=\u001b[39m main(mission, use_model, img_path, shp_path, box_path, old_weights)\n",
      "Cell \u001b[0;32mIn [29], line 74\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(mission, use_model, img_path, shp_path, box_path, old_weights)\u001b[0m\n\u001b[1;32m     69\u001b[0m model_tensorboard_callback \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mTensorBoard(log_dir\u001b[39m=\u001b[39mlog_dir, histogram_freq\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, write_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, write_images\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     70\u001b[0m model_callbacks \u001b[39m=\u001b[39m [model_checkpoint_callback, model_lrscheduler_callback,\n\u001b[1;32m     71\u001b[0m                     model_lrreduce_callback, model_earlystopping_callback,\n\u001b[1;32m     72\u001b[0m                     model_tensorboard_callback,]\n\u001b[0;32m---> 74\u001b[0m history \u001b[39m=\u001b[39m norm_train(model, optimizer, losses_func, total_stragegy[\u001b[39m'\u001b[39;49m\u001b[39mloss_weights\u001b[39;49m\u001b[39m'\u001b[39;49m], model_callbacks , model_metrics, \n\u001b[1;32m     75\u001b[0m                      data_path, img_path, shp_path, box_path, total_stragegy[\u001b[39m'\u001b[39;49m\u001b[39mimg_size\u001b[39;49m\u001b[39m'\u001b[39;49m], total_stragegy[\u001b[39m'\u001b[39;49m\u001b[39mnum_band\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m     76\u001b[0m                      total_stragegy[\u001b[39m'\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m'\u001b[39;49m], total_stragegy[\u001b[39m'\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m'\u001b[39;49m], dict_params[\u001b[39m'\u001b[39;49m\u001b[39mn_labels\u001b[39;49m\u001b[39m'\u001b[39;49m], total_stragegy[\u001b[39m'\u001b[39;49m\u001b[39msplit_ratios\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m     77\u001b[0m                      total_stragegy[\u001b[39m'\u001b[39;49m\u001b[39muse_test\u001b[39;49m\u001b[39m'\u001b[39;49m], total_stragegy[\u001b[39m'\u001b[39;49m\u001b[39muse_multi\u001b[39;49m\u001b[39m'\u001b[39;49m], old_weights, total_stragegy[\u001b[39m\"\u001b[39;49m\u001b[39mimg_size\u001b[39;49m\u001b[39m\"\u001b[39;49m], total_stragegy[\u001b[39m\"\u001b[39;49m\u001b[39mstride_crop\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     78\u001b[0m \u001b[39mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/SKM/WORK/ALL_CODE/WORK/Q/tutorial/../scripts/train.py:23\u001b[0m, in \u001b[0;36mnorm_train\u001b[0;34m(model, optimizer, loss, loss_weights, callbacks, model_metrics, data_path, img_path, shp_path, box_path, img_size, num_band, epochs, batch_size, num_class, split_ratios, use_test, use_multi, old_weights, size_crop, stride)\u001b[0m\n\u001b[1;32m     21\u001b[0m if os.path.exists(overlap_img) and os.path.exists(overlap_mask):\n\u001b[1;32m     22\u001b[0m     pass\n\u001b[0;32m---> 23\u001b[0m else:\n\u001b[1;32m     24\u001b[0m     mask_path = main_build_mask(img_path, shp_path)\n\u001b[1;32m     25\u001b[0m     crop_img_path = main_cut_img(img_path, box_path, out_path)\n",
      "File \u001b[0;32m~/SKM/WORK/ALL_CODE/WORK/Q/tutorial/../preprocess/build_mask.py:88\u001b[0m, in \u001b[0;36mmain_build_mask\u001b[0;34m(img_dir, shape_path)\u001b[0m\n\u001b[1;32m     85\u001b[0m path_create \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(parent, \u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     87\u001b[0m p_cnt \u001b[39m=\u001b[39m Pool(processes\u001b[39m=\u001b[39mcore)\n\u001b[0;32m---> 88\u001b[0m p_cnt\u001b[39m.\u001b[39;49mmap(partial(build_mask,img_dir\u001b[39m=\u001b[39;49mimg_dir,path_create\u001b[39m=\u001b[39;49mpath_create,path_shape\u001b[39m=\u001b[39;49mshape_path), list_id)\n\u001b[1;32m     89\u001b[0m p_cnt\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     90\u001b[0m p_cnt\u001b[39m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/anaconda3/envs/tmpp/lib/python3.8/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/anaconda3/envs/tmpp/lib/python3.8/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[0;31mRasterioIOError\u001b[0m: Read or write failed. PROJ: proj_identify: /home/skm/anaconda3/envs/tmpp/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation."
     ]
    }
   ],
   "source": [
    "# # # Khai báo tên task đang làm và tên model để lưu weights\n",
    "# mission = 'road_multi'\n",
    "# # mission = 'add_mangrove'\n",
    "# use_model = 'swin_unet' # greencover sử dụng att_unet\n",
    "\n",
    "\n",
    "# Duc Anh\n",
    "# Khai báo tên task đang làm và tên model để lưu weights\n",
    "mission = 'cloud_distribute_ducanh123aaz'\n",
    "# mission = 'add_mangrove'\n",
    "use_model = 'att_unet' # greencover sử dụng att_unet\n",
    "\n",
    "# Đường dẫn ảnh , box, label trong đó có các file phải cùng tên nhau và khác thư mục\n",
    "# Data tạo ra ra được lưu trong đường dẫn /home/quyet/DATA_ML/WorkSpace/segmentation/data/ + tên mission \n",
    "# Cấu trúc data train gồm 2 thư mục :\n",
    "# ---img_cut_crop: *.tif\n",
    "# ---mask_cut_crop: *.tif\n",
    "\n",
    "img_path = '/home/skm/SKM16/Data/ZZ/img'\n",
    "shp_path = '/home/skm/SKM16/Data/ZZ/label'\n",
    "box_path = '/home/skm/SKM16/Data/ZZ/box'\n",
    "old_weights = None\n",
    "\n",
    "# img_path = '/home/quyet/DATA_ML/WorkSpace/segmentation/tutorial/LuyenAll/images'\n",
    "# shp_path = '/home/quyet/DATA_ML/WorkSpace/segmentation/tutorial/LuyenAll/lables_water'\n",
    "# box_path = '/home/quyet/DATA_ML/WorkSpace/segmentation/tutorial/LuyenAll/boxs'\n",
    "# old_weights = '/home/quyet/DATA_ML/WorkSpace/segmentation/tutorial/LuyenAll/water_weights.h5'\n",
    "\n",
    "history = main(mission, use_model, img_path, shp_path, box_path, old_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13bfab8e-da05-47b7-a5ff-2d75f06fc8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/skm/SKM/WORK/ALL_CODE/WORK/Q/tutorial\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76c99c0c-ceb9-4a8e-a4a2-2326b8c8ab83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/skm/anaconda3/envs/tmpp/lib/python3.8/site-packages/osgeo/gdal.py'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from osgeo import gdal\n",
    "\n",
    "gdal.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadcbe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "'/home/skm/anaconda3/envs/tmpp/lib/python3.8/site-packages/osgeo/gdal.py'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tmpp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "da7ba9777add05ae4453e38436b53932ae391292d6a3122b5a580e0c8b9d2559"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
